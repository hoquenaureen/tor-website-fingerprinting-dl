{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XyANkPIeBbx6"},"outputs":[],"source":["# Get datasets from: https://drive.google.com/open?id=16w02wuMOqoLm6-YlM-rhAC1uZthO-2A3\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# authenticate user credentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# download datasets from GDrive\n","# downloaded = drive.CreateFile({'id': '1pghUuYyaYsuwDCA7Ow1Y601kGl6qPboL'})\n","# downloaded.GetContentFile('large_10_1000.zip')\n","# downloaded = drive.CreateFile({'id': '19wcJs9TdIwhT85TGdsaw1GV1ypbLnOpK'})\n","# downloaded.GetContentFile('small_10_100.zip')\n","downloaded = drive.CreateFile({'id': '1f-o1ygRlTZtvhbC9naP721khFJicbp-n'})\n","downloaded.GetContentFile('large_95_100.zip')\n","\n","# unzip the datasets\n","# !unzip -o small_10_100.zip\n","# !unzip -o large_10_1000.zip\n","!unzip -o large_95_100.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUGHSo_QFDW-"},"outputs":[],"source":["%tensorflow_version 2.x\n","%load_ext tensorboard\n","import numpy as np\n","from sklearn.utils import shuffle\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from tensorflow.keras.optimizers import RMSprop, Adam, Adamax\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n","from tensorflow.keras.regularizers import l2\n","from itertools import takewhile\n","from keras.utils.vis_utils import plot_model\n","\n","\n","import tensorflow as tf\n","#tf.logging.set_verbosity(tf.logging.ERROR)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_OSSyiQDz-F"},"outputs":[],"source":["\n","# Global Definitions\n","data_path=\"undefended/\"  # trace data path\n","\n","num_sites=95               # number of sites (max 95)\n","num_instances=100          # number of instances per site (max 100)\n","file_ext=\"\"                # trace file extension\n","max_length = 200          # maximum number of packet directions to use\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6o69Nza-8i0T"},"outputs":[],"source":["def condense(x):\n","  ol = len(x)\n","  x = list(filter(lambda a: a != 0, x))\n","  out = []\n","  while x:\n","    e = x[0]\n","    count = (len(list(takewhile(lambda a: a == e, x))))\n","    x = x[count:]\n","    out.append(count * e)\n","  return out\n","\n","\n","def get_data():\n","    \"\"\"\n","\n","    :return: a numpy ndarray of dimension (m x (n+1)) containing direction data\n","        loaded from the files, where `m` is the number of data samples and `n`\n","        is length of direction packets (restricted to 500 to consume less\n","        computation time and memory). The last column in the data contains the\n","        class labels of the `m` samples, which are the website numbers.\n","\n","    This function loads the data from the files and creates a numpy data matrix\n","    with each row as a data sample and the columns containing packet direction.\n","    \n","    The column data is condensed to decrease the size of the packet data in\n","    order to make LSTM classification easier. Packets in the form \n","    [-1,-1,-1,1,1,1,1] are instead saved as [-3,4]. The change in packet sizes\n","    with this condensation method being used is noted.\n","\n","    The last column of the data is the label, which is the website to which the\n","    instance belongs.\n","    \"\"\"\n","\n","    # read data from files\n","    print(\"loading data...\")\n","    data = []\n","    len_changes = []\n","    lens = []\n","    max_len_change = 0\n","    min_len_change = 1000000000\n","    for site in range(0, num_sites):\n","        for instance in range(0, num_instances):\n","            file_name = str(site) + \"-\" + str(instance)\n","            # Directory of the raw data\n","            with open(data_path + file_name + file_ext, \"r\") as file_pt:\n","                directions = []\n","                for line in file_pt:\n","                    x = line.strip().split('\\t')\n","                    directions.append(1 if float(x[1]) > 0 else -1)\n","                # Condense sequence data\n","                # Save old lengths\n","                old_len = len(directions)\n","                # old_sum = sum(directions)\n","                # Condese directions array\n","                directions = condense(directions)\n","                # record new lengths\n","                # new_sum = sum(directions)\n","                new_len = len(directions)\n","                lens.append(new_len)\n","                # save difference in length\n","                diff = old_len - new_len\n","                # Update max and min change values, if applicable\n","                if diff > max_len_change:\n","                  max_len_change = diff\n","                if diff < min_len_change:\n","                  min_len_change = diff\n","                len_changes.append(diff)\n","                # Pad/Condense directions array to max_len\n","                if len(directions) < max_length:\n","                    zend = max_length - len(directions)\n","                    directions.extend([0] * zend)\n","                elif len(directions) > max_length:\n","                    directions = directions[:max_length]\n","                # update large array with properly sized packet\n","                data.append(directions + [site])\n","    # Display min/max/average length changes\n","    print(f\"Maximum Length Change: {max_len_change}\")\n","    print(f\"Minimum Length Change: {min_len_change}\")\n","    print(f\"Average Length Change: {np.mean(len_changes)}\")\n","    print(f\"Average Condensed Length: {np.mean(lens)}\")\n","    print(\"done\")\n","    return np.array(data)\n","\n","\n","def split_data(X, Y, fraction=0.80, balance_dist=False):\n","    \"\"\"\n","    :param X: a numpy ndarray of dimension (m x n) containing data samples\n","    :param Y: a numpy ndarray of dimension (m x 1) containing labels for X\n","    :param fraction: a value between 0 and 1, which will be the fraction of\n","        data split into training and test sets. value of `fraction` will be the\n","        training data and the rest being test data.\n","    :param balance_dist: boolean value. The split is performed with ensured\n","        class balance if the value is true.\n","    :return: X_train, Y_train, X_test, Y_test\n","\n","    This function splits the data into training and test datasets.\n","    \"\"\"\n","    X, Y = shuffle(X, Y)\n","    m, n = X.shape\n","    split_index = int(round(m*fraction))\n","    if balance_dist:\n","        X_train = np.zeros(shape=(split_index, n))\n","        X_test = np.zeros(shape=(m-split_index, n))\n","        Y_train = np.zeros(shape=(split_index,))\n","        Y_test = np.zeros(shape=(m-split_index,))\n","        labels = np.unique(Y)\n","        ind1 = 0\n","        ind2 = 0\n","        for i in np.arange(labels.size):\n","            indices = np.where(Y == labels[i])[0]\n","            split = int(round(len(indices)*fraction))\n","\n","            X_train[ind1:ind1 + split, :] = X[indices[:split], :]\n","            X_test[ind2:ind2+(indices.size-split), :] = X[indices[split:], :]\n","\n","            Y_train[ind1:ind1 + split] = Y[indices[:split]]\n","            Y_test[ind2:ind2+(indices.size-split)] = Y[indices[split:]]\n","\n","            ind1 += split\n","            ind2 += indices.size-split\n","        X_train, Y_train = shuffle(X_train, Y_train)\n","        X_test, Y_test = shuffle(X_test, Y_test)\n","        return X_train, Y_train, X_test, Y_test\n","    return X[:split_index, :], Y[:split_index], \\\n","        X[split_index:, :], Y[split_index:]\n","\n","class LSTM:\n","    def __init__(self, num_features, num_classes):\n","        # We begin by defining the a empty stack. We'll use this for building our \n","        # network, later by layer.\n","        model = Sequential()\n","\n","        # add the LSTM layer\n","        # input units are condensed data length\n","        # dropout of 0.5\n","        # tanh activation function (although I think that's set by default)\n","        model.add(\n","            tf.keras.layers.LSTM(\n","                units=max_length, # input units as variable so we can test\n","                return_sequences=False,\n","                input_shape = (num_features, 1),\n","                recurrent_dropout=0.5,\n","                activation=\"tanh\" \n","            )\n","        )\n","\n","        # All LSTM units are connected to output the number of units as we\n","        # have websites. the \"softmax\" activation function is used to figure out\n","        # which class has the highest prediction\n","        model.add(\n","            tf.keras.layers.Dense(\n","                units=num_classes, # number of websites\n","                activation='softmax' # Softmax Activation Function\n","            )\n","        )\n","\n","        # Compile the model\n","        model.compile(\n","            loss=tf.keras.losses.categorical_crossentropy, # loss function\n","            optimizer=Adam(learning_rate=0.001),\n","            metrics=['accuracy']) # reporting metric\n","        self.model = model\n","        plot_model(model, to_file='model_plot.png', show_shapes=True)\n","        # Display a summary of the models structure\n","        print(self.model.summary())\n","\n","    def fit(self, x_train, y_train, batch_size, epochs, verbose):\n","        tboard_cb = TensorBoard(log_dir='./graph', histogram_freq=0,\n","                        write_graph=True, write_images=True)\n","        early_stopping_cb = EarlyStopping(monitor=\"val_loss\", patience=3)\n","        # Train the LSTM on the training data\n","        self.model.fit(\n","\n","            # Training data\n","            x_train, y_train,\n","                            \n","            # Number of samples to work through before updating the \n","            # internal model parameters via back propagation. The \n","            # higher the batch, the more memory you need.\n","            batch_size=batch_size, \n","\n","            # An epoch is an iteration over the entire training data.\n","            epochs=epochs, \n","            \n","            # The model will set apart his fraction of the training \n","            # data, will not train on it, and will evaluate the loss\n","            # and any model metrics on this data at the end of \n","            # each epoch.\n","            validation_split=0.2,\n","            \n","            verbose=verbose,\n","\n","            callbacks=[tboard_cb, early_stopping_cb]\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0eI_uGf60e4"},"outputs":[],"source":["def main():\n","\n","    # Load the data and create X and Y matrices\n","    data = get_data()\n","    num_features = data.shape[1] - 1\n","    X = data[:, :num_features]\n","    Y = data[:, -1]\n","\n","    # split the data into training and test set\n","    X_train, Y_train, X_test, Y_test = split_data(X, Y, 0.85, balance_dist=True)\n","    X_train = np.expand_dims(X_train, axis=2)\n","    X_test = np.expand_dims(X_test, axis=2)\n","    Y_train = to_categorical(Y_train)\n","    Y_test = to_categorical(Y_test)\n","\n","    # instantiate the CNN model and train on the data\n","    # model = CNN(num_features, Y_train.shape[1])\n","    # model.fit(X_train, Y_train, batch_size=25, epochs=500, verbose=1)\n","\n","    # instantiate the LSTM model and train on the data\n","    model = LSTM(num_features, Y_train.shape[1])\n","    model.fit(X_train, Y_train, batch_size=64, epochs=50, verbose=1)\n","    \n","    # Evaluate the trained CNN model on test data and print the accuracy\n","    score = model.model.evaluate(X_test, Y_test, batch_size=100)\n","    print(\"Test accuracy: \", round(score[1]*100, 2))\n","    print(\"Test loss: \", round(score[0], 2))\n","\n","    # Get LSTM Model Predictions for test data\n","    # from sklearn.metrics import classification_report\n","    # predicted_classes = (model.predict(X_test) > 0.5).astype(\"int32\")\n","    # print(classification_report(Y_test, predicted_classes, \n","    # target_names=class_names))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tS0LokRdFL7_"},"outputs":[],"source":["if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEQo4qA8eZKj"},"outputs":[],"source":["%tensorboard --logdir graph/"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"code.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}